{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41502c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import matplotlib.dates as mdates\n",
    "import csv\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading template swarm 2023-08-27\n",
    "file_list1 = glob.glob(\"/data/fast1/veronica-scratch-rainier-downsampling/drive1_ds/new_decimator_2023-08-27*\")\n",
    "\n",
    "\n",
    "file_list = file_list1 #+ file_list2 + file_list3 + file_list4 + file_list5 + file_list6 + file_list7 \n",
    "file_list.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2cc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "chan_min = 1000\n",
    "chan_max = 2500\n",
    "#chan_min = 0\n",
    "#chan_max = -1\n",
    "channel_number = chan_max -chan_min\n",
    "low_cut1 = 2\n",
    "hi_cut1 = 9.0\n",
    "#fs=attrs['MaximumFrequency']*2\n",
    "fs = 20\n",
    "samples_per_file = 60*fs\n",
    "\n",
    "number_of_template = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd1af0",
   "metadata": {},
   "source": [
    "# Buiding Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first version, just one event\n",
    "\n",
    "\n",
    "# Ruta del archivo HDF5\n",
    "#template1\n",
    "\n",
    "template_path = '/data/fast1/veronica-scratch-rainier-downsampling/drive1_ds/new_decimator_2023-08-27_12.37.00_UTC.h5'\n",
    "\n",
    "#template2 \n",
    "#rawdata_path = '/data/data4/veronica-scratch-rainier/drive1_ds/new_decimator_2023-08-27_11.38.00_UTC.h5'\n",
    "\n",
    "\n",
    "# Abrir el archivo HDF5\n",
    "with h5py.File(template_path, 'r') as rawdata_file:\n",
    "    # Acceder a los datos que deseas\n",
    "    rawdata = rawdata_file['Acquisition/Raw[0]/RawData']\n",
    "\n",
    "    # convert the template in a numpy array\n",
    "    this_template = np.array(rawdata[:, chan_min:chan_max])\n",
    "    #closing file\n",
    "    rawdata_file.close()\n",
    "    \n",
    "# filtering\n",
    "b, a = butter(2, (low_cut1, hi_cut1), 'bp', fs=fs)\n",
    "template_filt = filtfilt(b, a, this_template, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder for the figures\n",
    "\n",
    "# Extract the part of the filename containing the date and time\n",
    "filename = os.path.basename(template_path)\n",
    "date_time_match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}\\.\\d{2}\\.\\d{2})', filename)\n",
    "if date_time_match:\n",
    "    date_time = date_time_match.group(1)\n",
    "else:\n",
    "    raise ValueError(\"No date and time found in the filename.\")\n",
    "\n",
    "# Separate date and time\n",
    "date = date_time[:10]\n",
    "time_minutes = date_time[11:16].replace('.', '-')\n",
    "\n",
    "# Get the base directory\n",
    "base_directory = '/home/velgueta/notebooks/RainierDas/Plot-Rainier-TM'\n",
    "\n",
    "# Full path of the folder to create\n",
    "folder_path = os.path.join(base_directory, f\"{date}_{time_minutes}\")\n",
    "\n",
    "# Check if the folder already exists, if not, create it\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{date}_{time_minutes}' created in '{base_directory}'\")\n",
    "else:\n",
    "    print(f\"Folder '{date}_{time_minutes}' already exists in '{base_directory}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a22fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutting file of template\n",
    "\n",
    "#template2, template = template_filt[120:140,:]\n",
    "template = template_filt[30:50,:]\n",
    "plt.imshow(template.T, cmap='seismic',aspect='auto',vmin=-0.05,vmax=0.05)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d613a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate(s1,s2,mode=\"same\"):\n",
    "\n",
    "    # throw an error of input sizes are inconsistent\n",
    "    if s1.shape != s2.shape:\n",
    "        raise ValueError(\"s1 and s2 must have the same size!\")\n",
    "\n",
    "    # get fft size\n",
    "    sz = s1.shape[0]\n",
    "    n_bits = 1+int(np.log2(2*sz-1))\n",
    "    fft_sz = 2**n_bits\n",
    "\n",
    "    # take FFT along time axis for both\n",
    "    fft_s1 = np.fft.fft(s1, fft_sz, axis=0)\n",
    "    fft_s2 = np.fft.fft(s2, fft_sz, axis=0)\n",
    "\n",
    "    # take complex conjugate of second signal\n",
    "    fft_s2_conj = np.conj(fft_s2)\n",
    "\n",
    "    # multiply to get correlation function\n",
    "    corr_fft = fft_s1*fft_s2_conj\n",
    "\n",
    "    # take inverse fourier transform\n",
    "    corr = np.fft.ifft(corr_fft, axis=0)\n",
    "\n",
    "    # normalize using the magnitude of both input data\n",
    "    norm1 = np.linalg.norm(s1,axis=0)\n",
    "    norm2 = np.linalg.norm(s2,axis=0)\n",
    "    norm_factor = norm1*norm2\n",
    "    corr = np.vstack((corr[-(sz-1) :], corr[:sz]))\n",
    "    norm_corr = np.real(corr) / norm_factor\n",
    "\n",
    "    # return desired part of correlation function\n",
    "    if mode == \"full\":\n",
    "        pass\n",
    "    elif mode == \"same\":\n",
    "        norm_corr = norm_corr[int(sz/2):-int(sz/2)+1]\n",
    "    return norm_corr\n",
    "\n",
    "\n",
    "def window_and_correlate(template,data):\n",
    "\n",
    "    # define container\n",
    "    all_corr = []\n",
    "\n",
    "    # get some helpful values\n",
    "    window_length = template.shape[0]\n",
    "    num_windows = int(data.shape[0]/window_length)\n",
    "\n",
    "    # iterate through time windows\n",
    "    for i in range(num_windows):\n",
    "\n",
    "        # pull out a time window of data\n",
    "        start_index = i*window_length\n",
    "        end_index = start_index + window_length\n",
    "        window = data[start_index:end_index,:]\n",
    "\n",
    "        # call cross correlation function\n",
    "        corr = correlate(template,window)\n",
    "\n",
    "        # save value\n",
    "        all_corr.append(corr)\n",
    "\n",
    "    # reshape output\n",
    "    all_corr = np.stack(all_corr)\n",
    "\n",
    "    return all_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6fd2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating directories for each experiments\n",
    "\n",
    "corrs = \"corrs_templatetest\"\n",
    "freqs = \"freqs-2-9\"\n",
    "\n",
    "\n",
    "daystart = file_list[0]\n",
    "dayend = file_list[-1]\n",
    "\n",
    "# Extract date, hour, and minute portion from file path, excluding \"new\" and \"decimator\"\n",
    "daystart_parts = os.path.basename(daystart).split('_')\n",
    "daystart_date_time = \"_\".join(daystart_parts[2:4])  # Extract the date and time parts, excluding \"new_decimator\" and \"UTC.h5\"\n",
    "\n",
    "dayend_parts = os.path.basename(dayend).split('_')\n",
    "dayend_date_time = \"_\".join(dayend_parts[2:4])  # Extract the date and time parts, excluding \"new_decimator\" and \"UTC.h5\"\n",
    "\n",
    "# Base directory to save files\n",
    "base_directory = '/data/data4/veronica-scratch-rainier/npyfiles/'\n",
    "\n",
    "# Create the output folder name\n",
    "folder_output = os.path.join(base_directory, f\"{corrs}_{freqs}_{chan_min}_{chan_max}_{daystart_date_time}_{dayend_date_time}\")\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(folder_output):\n",
    "    os.mkdir(folder_output)\n",
    "    print(f\"Directory '{folder_output}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{folder_output}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no funciono la corrcorrelacion?? parece que si \n",
    "\n",
    "\n",
    "inicio_tiempo = time.perf_counter()\n",
    "\n",
    "\n",
    "datetimestotal = []\n",
    "corrs3_list = []\n",
    "all_timestamps = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_list)):\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        try:\n",
    "            # Lee los datos del archivo y realiza las operaciones necesarias\n",
    "            raw_data = np.array(f['Acquisition/Raw[0]/RawData'][:, chan_min:chan_max])\n",
    "            timestamps = np.array(f['Acquisition/Raw[0]/RawDataTime'])  # tiempo en microsegundos desde 1970-01-01\n",
    "            data_filt = filtfilt(b, a, raw_data, axis=0)\n",
    "\n",
    "            all_timestamps.extend(timestamps)\n",
    "\n",
    "            # Ajusta la longitud de los datos al número samples_per_file si es necesario\n",
    "            if data_filt.shape != template.shape:\n",
    "                data_filt = data_filt[:samples_per_file, :]\n",
    "\n",
    "            # Calcula correlaciones incluso si la longitud no coincide\n",
    "            corrs = window_and_correlate(template, data_filt)\n",
    "            corrs2 = corrs.reshape((int(samples_per_file), channel_number))\n",
    "            corrs3 = np.sum(corrs2, axis=1) / channel_number\n",
    "            corrs3_list.append(corrs3)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar el archivo {file}: {e}. Se omite este archivo.\")\n",
    "            continue\n",
    "        finally:\n",
    "            f.close()  \n",
    "            # Asegura cerrar el archivo incluso si hay un error\n",
    "            output_folder = folder_output   \n",
    "     \n",
    "        \n",
    "        for i, corrs3 in enumerate(corrs3_list):\n",
    "            outfile_name = os.path.join(output_folder, f'corrs3_{i}.npy')\n",
    "            np.save(outfile_name, corrs3)\n",
    "          \n",
    "                    \n",
    "# Registra el tiempo de finalización\n",
    "fin_tiempo = time.perf_counter()\n",
    "\n",
    "# Calcula el tiempo de ejecución\n",
    "finaltime = fin_tiempo - inicio_tiempo\n",
    "\n",
    "print(f\"code took {finaltime} seconds.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6517bf",
   "metadata": {},
   "source": [
    "# graficando los archivos corrs3.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#time steps\n",
    "timesteps = len(file_list)* samples_per_file\n",
    "# Extraer las fechas de inicio y fin del nombre de la carpeta de salida\n",
    "folder_parts = os.path.basename(folder_output).split('_')\n",
    "start_datetime = \"_\".join(folder_parts[-4:-2])  # Extract date and time from the second-to-last and last elements\n",
    "end_datetime = \"_\".join(folder_parts[-2:])  # Extract date and time from the last two elements\n",
    "\n",
    "# Convertir las cadenas de tiempo a objetos datetime\n",
    "start_datetime = pd.to_datetime(start_datetime, format=\"%Y-%m-%d_%H.%M.%S\")\n",
    "end_datetime = pd.to_datetime(end_datetime, format=\"%Y-%m-%d_%H.%M.%S\")\n",
    "\n",
    "# Crear un rango de fechas con 1728000 puntos\n",
    "time_range = pd.date_range(start=start_datetime, end=end_datetime, periods=timesteps)\n",
    "\n",
    "\n",
    "# Juntar output_folder con folder_output\n",
    "output_folder = folder_output\n",
    "\n",
    "# Lista para almacenar las correlaciones cargadas de los archivos\n",
    "corrs3_list = []\n",
    "\n",
    "# Cargar los archivos npy y almacenar las correlaciones en corrs3_list\n",
    "for i, file in enumerate(os.listdir(output_folder)):\n",
    "    if file.startswith(\"corrs3_\") and file.endswith(\".npy\"):\n",
    "        filepath = os.path.join(output_folder, file)\n",
    "        corrs3 = np.load(filepath)\n",
    "        corrs3_list.append(corrs3)\n",
    "\n",
    "# Unir todos los arreglos de correlaciones en uno solo\n",
    "correlaciones_totales = np.concatenate(corrs3_list)\n",
    "\n",
    "# Graficar las correlaciones en función del tiempo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_range, correlaciones_totales)\n",
    "plt.xlabel('Time (UTC)')\n",
    "plt.ylabel('Correlation Value')\n",
    "plt.title('Total Correlation Results')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timestam to datetime\n",
    "\n",
    "import datetime\n",
    "# Convertir las marcas de tiempo en objetos datetime\n",
    "datetime_objects = [datetime.datetime.fromtimestamp(ts / 1e6) for ts in all_timestamps]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datetime_objects)*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14756299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista que contenga todos los puntos de correlación\n",
    "corrs_all = []\n",
    "for i, corrs3 in enumerate(corrs3_list):\n",
    "    corrs_all.extend(corrs3)\n",
    "\n",
    "\n",
    "corrs_all = np.array(corrs_all)\n",
    "datetimes_all = np.array(datetime_objects)\n",
    "\n",
    "# Supongamos que tienes un arreglo datetimes_array con 360,000 elementos\n",
    "# y un arreglo corrs3_list que contiene 60 arreglos de 6000 puntos cada uno\n",
    "\n",
    "# Definir la zona horaria original (supongamos que es Seattle)\n",
    "zona_horaria_original = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "# Convertir datetimes_array a UTC time\n",
    "utc_timezone = pytz.timezone('UTC')\n",
    "dates_times_utc = [date_time_pacific.astimezone(utc_timezone) for date_time_pacific in datetimes_all]\n",
    "\n",
    "\n",
    "# Unir todos los arreglos de correlaciones en uno solo\n",
    "correlaciones_totales = np.concatenate(corrs3_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_times_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58798549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making nice plot\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(dates_times_utc[0:len(correlaciones_totales)], correlaciones_totales[0:len(dates_times_utc)], linestyle='-', color='k', linewidth=1)\n",
    "plt.ylim((0, 0.2))\n",
    "#plt.xlim((, 90))\n",
    "plt.xlabel(\"Time (UTC) since 27/8\",fontsize = 20)\n",
    "plt.ylabel(\"Cross correlation value\",fontsize = 20)\n",
    "plt.title(\" Template using Template2 event_downsampling\", fontsize = 20)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "# Personalizar los marcadores del eje x\n",
    "plt.xticks(rotation=45)  # Rotar las etiquetas del eje x en 45 grados\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))  # Formato de fecha y hora\n",
    "\n",
    "# Agregar líneas verticales en cada elemento de la lista horas_datetime\n",
    "#for hour_datetime in horas_datetime:\n",
    "##\n",
    "    #plt.axvline(x=hour_datetime, color='orange', linestyle='--')\n",
    "    #plt.text(horas_datetime[10], 0.15, 'Quakes detected by PNSN', color='orange', rotation=0, fontsize=15, verticalalignment='bottom')\n",
    "\n",
    "#plt.savefig('Mount-Rainier_5daytocompare_downsampling_1a98.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c17279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing method for define a thresold\n",
    "thresholds = [0.5, 0.1, 0.05, 0.03]\n",
    "\n",
    "# Iterate over each threshold\n",
    "for threshold in thresholds:\n",
    "    # Find indices where values exceed the threshold\n",
    "    indices_above_threshold = np.where(np.abs(correlaciones_totales) > threshold)[0]\n",
    "\n",
    "    # Find differences between consecutive indices\n",
    "    diff_indices = np.diff(indices_above_threshold)\n",
    "\n",
    "    # Find changes in differences indicating a new group\n",
    "    group_changes = np.where(diff_indices > 40)[0]\n",
    "\n",
    "    # Group indices into contiguous lists\n",
    "    detection_groups = np.split(indices_above_threshold, group_changes + 1)\n",
    "\n",
    "    # Count the number of detections for the current threshold\n",
    "    count_above_threshold = len(detection_groups)\n",
    "\n",
    "    # Print the result for the current threshold\n",
    "    print(f\"Detections for threshold downsampled {threshold}: {count_above_threshold}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860f290",
   "metadata": {},
   "source": [
    "# testing method for define a thresold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55551c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poniendole el nombre del archivo txt, con el template que se uso.\n",
    "\n",
    "# Rawdata file path\n",
    "#rawdata_path = '/data/data4/veronica-scratch-rainier/drive1_ds/new_decimator_2023-08-27_10.10.00_UTC.h5'\n",
    "\n",
    "# Extracting date from the file name using regular expressions\n",
    "match = re.search(r'new_decimator_(\\d{4}-\\d{2}-\\d{2}_\\d{2}.\\d{2}.\\d{2})', template_path)\n",
    "if match:\n",
    "    date_with_hm = match.group(1)\n",
    "    print(\"Date extracted from the file name:\", date_with_hm)\n",
    "\n",
    "    # Set the output folder for text files\n",
    "    output_folder = 'txtfiles'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Text file name based on the extracted date\n",
    "    output_file_path_TM = os.path.join(output_folder, f'results-TM_{date_with_hm}.txt')\n",
    "    print(\"Text file name:\", output_file_path_TM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf58281",
   "metadata": {},
   "source": [
    "# Creating the txt files saving thresolds, numbers of dedections and dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "thresholds = [0.5, 0.1, 0.05, 0.04, 0.035, 0.03]  # Example thresholds\n",
    "with open(output_file_path_TM, 'w') as file:\n",
    "    # Write headers\n",
    "    file.write(\"Threshold\\tNumber of Detections\\tDetection Times (UTC)\\n\")\n",
    "\n",
    "    # Iterate over results for each threshold\n",
    "    for threshold in thresholds:\n",
    "        indices_above_threshold = np.where(np.abs(correlaciones_totales) > threshold)[0]\n",
    "        diff_indices = np.diff(indices_above_threshold)\n",
    "        group_changes = np.where(diff_indices > 20)[0]\n",
    "        detection_groups = np.split(indices_above_threshold, group_changes + 1)\n",
    "\n",
    "        # Iterate over each detection group\n",
    "        for group in detection_groups:\n",
    "            # Tomar solo la primera fecha en cada grupo\n",
    "            first_detection_time_utc = datetime.datetime.utcfromtimestamp(dates_times_utc[group[0]].timestamp()).strftime('%Y-%m-%d_%H.%M')\n",
    "            #first_detection_time_utc = datetime.datetime.utcfromtimestamp(dates_times_utc[group[0]].timestamp()).strftime('%Y-%m-%d_%H.%M.%S')\n",
    "            # Escribir los datos en una fila separados por tabulaciones\n",
    "            file.write(f\"{threshold}\\t{len(detection_groups)}\\t{first_detection_time_utc}\\n\")\n",
    "\n",
    "print(f\"The results have been saved in {output_file_path_TM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c95371",
   "metadata": {},
   "source": [
    "# Making a nice plot of thrsolds versus detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo de resultados\n",
    "results_file_path = output_file_path_TM\n",
    "\n",
    "# Listas para almacenar los datos\n",
    "thresholds = []\n",
    "num_detections = []\n",
    "\n",
    "# Cargar datos desde el archivo results.txt\n",
    "with open(results_file_path, 'r') as file:\n",
    "    # Leer las líneas del archivo\n",
    "    lines = file.readlines()\n",
    "\n",
    "    # Iterar sobre las líneas, omitiendo la primera que son los encabezados\n",
    "    for line in lines[1:]:\n",
    "        # Dividir la línea en columnas\n",
    "        columns = line.strip().split('\\t')\n",
    "\n",
    "        # Obtener umbral y número de detecciones\n",
    "        threshold = float(columns[0])\n",
    "        detections = int(columns[1])\n",
    "\n",
    "        # Agregar datos a las listas\n",
    "        thresholds.append(threshold)\n",
    "        num_detections.append(detections)\n",
    "\n",
    "# Configuración de estilo del gráfico\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.viridis(np.linspace(0, 1, 12)))\n",
    "\n",
    "# Graficar umbral versus número de detecciones\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Graficar umbral versus número de detecciones\n",
    "plt.scatter(thresholds, num_detections, marker='o', label='Data Points')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Number of Detections')\n",
    "plt.title('Threshold vs Number of Detections')\n",
    "\n",
    "# Ajustar los ejes automáticamente para mostrar el gráfico completo\n",
    "plt.autoscale()\n",
    "\n",
    "\n",
    "plt.xscale('linear')  # Escala lineal en el eje x\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c39a06",
   "metadata": {},
   "source": [
    "# line that compare the results_xxx.txt and the usgs txt made by me (Vero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the function to parse a date string into a datetime object\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%Y-%m-%d_%H.%M.%S')\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Define the time difference allowed (5 seconds)\n",
    "time_difference = timedelta(seconds=6)\n",
    "\n",
    "# Function to read dates and values from a file and return a list of tuples containing both\n",
    "def read_dates_and_values_from_file(file_path):\n",
    "    dates_and_values = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        next(f)  # Skip header line\n",
    "        for line in f:\n",
    "            elements = line.strip().split('\\t')\n",
    "            if len(elements) >= 2:  # Ensure there are at least two elements\n",
    "                value = elements[0]  # Extract the value from the first column\n",
    "                date_str = elements[-1]  # Extract the date from the last column\n",
    "                date = parse_date(date_str)\n",
    "                if date is not None:\n",
    "                    dates_and_values.append((value, date))\n",
    "    return dates_and_values\n",
    "\n",
    "# Read dates and values from the converted_dates_and_values_rainier_5days.txt file\n",
    "converted_dates_and_values = read_dates_and_values_from_file('converted_dates_and_values_rainier_5days.txt')\n",
    "\n",
    "# Initialize lists to store common dates and unmatched dates\n",
    "common_dates = []\n",
    "unmatched_dates = []\n",
    "\n",
    "\n",
    "\n",
    "# New output filename\n",
    "output_filename = f\"match_for_{date_with_hm}.txt\"\n",
    "output_file_path = os.path.join('txtfiles', output_filename)\n",
    "\n",
    "# Read dates from the results file and compare with converted dates\n",
    "with open(output_file_path_TM, 'r') as f:\n",
    "    next(f)  # Skip header line\n",
    "    for line in f:\n",
    "        columns = line.strip().split('\\t')\n",
    "        if len(columns) >= 3:  # Ensure there are at least three elements in the list\n",
    "            threshold_value = columns[0]  # Extract threshold value from the first column\n",
    "            detection_times = columns[-1].split(',')  # Extract detection times from the last column\n",
    "            for detection_time in detection_times:\n",
    "                result_date = parse_date(detection_time)  # Convert detection time to a datetime object\n",
    "                if result_date is not None:  # Check if result_date is not None\n",
    "                    # Check if the result_date has a match in converted_dates_and_values\n",
    "                    matched = False\n",
    "                    for value, converted_date in converted_dates_and_values:\n",
    "                        if abs(result_date - converted_date) <= time_difference:\n",
    "                            common_dates.append((threshold_value, result_date, value))\n",
    "                            matched = True\n",
    "                            break\n",
    "                    if not matched:\n",
    "                        unmatched_dates.append((threshold_value, result_date))\n",
    "\n",
    "# Write common and unmatched dates to the new output file in the 'txtfiles' directory\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(\"Threshold\\tResult Date\\tID Value\\tMatch Status\\n\")\n",
    "    for threshold_value, result_date, id_value in common_dates:\n",
    "        f.write(f\"{threshold_value}\\t{result_date.strftime('%Y-%m-%d_%H.%M.%S')}\\t{id_value}\\tUGSGcatalog\\n\")\n",
    "    for threshold_value, result_date in unmatched_dates:\n",
    "        f.write(f\"{threshold_value}\\t{result_date.strftime('%Y-%m-%d_%H.%M.%S')}\\tX\\tUnmatched\\n\")\n",
    "\n",
    "print(f\"Output file '{output_file_path}' has been generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define paths and folders\n",
    "txt_file_path = output_file_path\n",
    "files_folder_path = '/data/data4/veronica-scratch-rainier/drive1_ds'\n",
    "plots_folder = 'Plot-Rainier-TM'\n",
    "\n",
    "# Regular expression pattern to search for dates in the correct format\n",
    "date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2})_(\\d{2})\\.(\\d{2})')\n",
    "\n",
    "# Extract information from output.txt\n",
    "result_dates = []\n",
    "thresholds = []\n",
    "id_values = []\n",
    "match_statuses = []\n",
    "\n",
    "with open(output_file_path, 'r') as txt_file:\n",
    "    for line in txt_file:\n",
    "        columns = line.strip().split('\\t')\n",
    "        threshold = columns[0]\n",
    "        result_date = columns[1]\n",
    "        id_value = columns[2]\n",
    "        match_status = columns[3]\n",
    "        result_dates.append(result_date)\n",
    "        thresholds.append(threshold)\n",
    "        id_values.append(id_value)\n",
    "        match_statuses.append(match_status)\n",
    "\n",
    "# Define the threshold to search\n",
    "threshold_to_search = '0.04'\n",
    "\n",
    "# Create the folder for plots if it doesn't exist\n",
    "if not os.path.exists(plots_folder):\n",
    "    os.makedirs(plots_folder)\n",
    "\n",
    "# Create a folder to hold all the plots\n",
    "all_plots_folder = os.path.join(plots_folder, f'{threshold_to_search}_{date_with_hm}')\n",
    "if not os.path.exists(all_plots_folder):\n",
    "    os.makedirs(all_plots_folder)\n",
    "\n",
    "# Iterate over the result dates and search for corresponding files with the specific threshold\n",
    "for i, result_date in enumerate(result_dates):\n",
    "    threshold = thresholds[i]\n",
    "    id_value = id_values[i]\n",
    "    match_status = match_statuses[i]\n",
    "    match = date_pattern.match(result_date)\n",
    "    if match:\n",
    "        date_part = match.group(1)\n",
    "        hour_part = match.group(2)\n",
    "        minute_part = match.group(3)\n",
    "        date_to_search = f\"{date_part}_{hour_part}.{minute_part}\"\n",
    "        if threshold == threshold_to_search:\n",
    "            found = False\n",
    "            for file_name in os.listdir(files_folder_path):\n",
    "                if date_to_search in file_name:\n",
    "                    print(f\"Found a file with the searched date: {file_name}\")\n",
    "                    found = True\n",
    "                    \n",
    "                    # Process the found file\n",
    "                    chan_min =0\n",
    "                    chan_max = -1\n",
    "                    data_file_path = os.path.join(files_folder_path, file_name)\n",
    "                    data_file = h5py.File(data_file_path, 'r')\n",
    "                    this_data = np.array(data_file['Acquisition/Raw[0]/RawData'][:, chan_min:chan_max])\n",
    "                    this_time = np.array(data_file['Acquisition/Raw[0]/RawDataTime'])\n",
    "                    attrs = dict(data_file['Acquisition'].attrs)\n",
    "                    data_file.close()\n",
    "\n",
    "                    low_cut1 = 2\n",
    "                    hi_cut1 = 9\n",
    "                    fs = 20\n",
    "                    b, a = butter(2, (low_cut1, hi_cut1), 'bp', fs=fs)\n",
    "                    data_filt = filtfilt(b, a, this_data, axis=0)\n",
    "\n",
    "                    date_format = mdates.DateFormatter('%H:%M:%S')\n",
    "                    x_lims = mdates.date2num(this_time)\n",
    "                    x_max = data_filt.shape[1] * attrs['SpatialSamplingInterval'] / 1000\n",
    "                    dx = x_max / data_filt.shape[1]\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(20,10))\n",
    "                    plt.imshow(data_filt.T, cmap='seismic', aspect='auto', vmin=-0.05, vmax=0.05, extent=[x_lims[0], x_lims[-1], x_max, 0])\n",
    "                    plt.xlabel(\"Time UTC\", fontsize=25)\n",
    "                    plt.ylabel(\"Optical distance (km)\", fontsize=25)\n",
    "                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "                    ax.xaxis_date()\n",
    "                \n",
    "                    # Set font size for time and channel axis labels\n",
    "                    plt.xticks(fontsize=20)\n",
    "                    plt.yticks(fontsize=20)\n",
    "                    \n",
    "                    # Set title including threshold, ID value, and match status\n",
    "                    plt.title(f'Threshold: {threshold}, ID: {id_value}, Match Status: {match_status}', fontsize=20)\n",
    "                    \n",
    "                    # Save the plot in the all_plots_folder\n",
    "                    full_path = os.path.join(all_plots_folder, f'{file_name}.png')\n",
    "                    plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "                    break\n",
    "\n",
    "            if not found:\n",
    "                print(f\"No matching file found for the searched date: {date_to_search}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86518a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the saved plots\n",
    "\n",
    "#plots_folder = f\"Plot-Rainier-TM/{threshold_to_search}_{date_with_hm}\"\n",
    "#plots_folder = \"/data/data4/veronica-scratch-rainier/plot-TM-results/0.04_2023-08-27_09.58.00\"\n",
    "#plots_folder = \"Plot-Rainier-TM\"\n",
    "# Iterate over the saved plots and display them\n",
    "#for file_name in os.listdir(plots_folder):\n",
    "#    if file_name.endswith('.png'):\n",
    "#        # Open the image file\n",
    "#        image_path = os.path.join(plots_folder, file_name)\n",
    "#        image = plt.imread(image_path)\n",
    "        \n",
    "        # Display the image\n",
    "#        plt.figure(figsize=(10, 6))\n",
    "#        plt.imshow(image)\n",
    "#        plt.title(file_name)\n",
    "#        plt.axis('off')  # Turn off axis labels\n",
    "#        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Obspy+LibComtCat)",
   "language": "python",
   "name": "python-obspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
