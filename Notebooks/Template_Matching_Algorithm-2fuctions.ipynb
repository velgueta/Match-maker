{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41502c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'templatematching'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtemplatematching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'templatematching'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import matplotlib.dates as mdates\n",
    "import csv\n",
    "import re\n",
    "from templatematching import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data to run\n",
    "\n",
    "def get_file_list(base_path,start_date,end_date):\n",
    "    file_list = []\n",
    "    actual_date = start_date\n",
    "    \n",
    "    while actual_date <= end_date:\n",
    "        #new_decimator is because is downsample data, usually is just decimator\n",
    "        search_pattern = f\"{base_path}/new_decimator_{actual_date}*\"\n",
    "        file_list.extend(glob.glob(search_pattern))\n",
    "        actual_date = increase_date(actual_date)\n",
    "        file_list.sort()\n",
    "    return file_list\n",
    "    \n",
    "# defining the other fuction to increment the date\n",
    "def increase_date(date_str):\n",
    "    from datetime import datetime,timedelta\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    date = datetime.strptime(date_str,date_format)\n",
    "    next_date = date + timedelta(days=1)\n",
    "    return next_date.strftime(date_format)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing fuctions\n",
    "base_path = \"/data/fast1/veronica-scratch-rainier-downsampling/drive1_ds\"\n",
    "start_date = \"2023-08-25\"\n",
    "end_date = \"2023-08-28\"\n",
    "file_list = get_file_list(base_path,start_date,end_date)\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd1647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block de prueba para ver como cambia la fecha de inicio\n",
    "chan_min=0\n",
    "chan_max=-1\n",
    "\n",
    "#data_file = h5py.File('/data/data5/Converted/whidbey_2022-05-17_08-39-00_UTC_014654.h5','r')\n",
    "data_file = h5py.File('/data/fast1/veronica-scratch-rainier-downsampling/drive1_ds/new_decimator_2023-08-25_00.00.00_UTC.h5','r')\n",
    "#data_file = h5py.File('/data/fast0/dominik-scratch/Shared/rainier_earthquake_2024_02_07/decimator_2024-02-07_19.11.00_UTC.h5','r')\n",
    "\n",
    "this_data0 = np.array(data_file['Acquisition/Raw[0]/RawData'][:,chan_min:chan_max])\n",
    "this_time0 = np.array(data_file['Acquisition/Raw[0]/RawDataTime'])\n",
    "            \n",
    "attrs=dict(data_file['Acquisition'].attrs)\n",
    "\n",
    "data_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block de prueba para ver como cambia la fecha de inicio\n",
    "chan_min=0\n",
    "chan_max=-1\n",
    "\n",
    "#data_file = h5py.File('/data/data5/Converted/whidbey_2022-05-17_08-39-00_UTC_014654.h5','r')\n",
    "data_file = h5py.File('/data/fast1/veronica-scratch-rainier-downsampling/drive1_ds/new_decimator_2023-08-31_23.59.00_UTC.h5','r')\n",
    "#data_file = h5py.File('/data/fast0/dominik-scratch/Shared/rainier_earthquake_2024_02_07/decimator_2024-02-07_19.11.00_UTC.h5','r')\n",
    "\n",
    "this_data1 = np.array(data_file['Acquisition/Raw[0]/RawData'][:,chan_min:chan_max])\n",
    "this_time1 = np.array(data_file['Acquisition/Raw[0]/RawDataTime'])\n",
    "            \n",
    "attrs=dict(data_file['Acquisition'].attrs)\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Marca de tiempo en PT (hora del Pacífico)\n",
    "timestamp_pt0 = this_time0[0]\n",
    "timestamp_pt1 = this_time1[0]\n",
    "\n",
    "# Diferencia horaria entre PT y UTC en segundos\n",
    "pt_offset_seconds = -25200  # Pacific Time (PT) is 8 hours behind UTC\n",
    "\n",
    "# Convertir la marca de tiempo de PT a UTC\n",
    "timestamp_utc0 = timestamp_pt0 + pt_offset_seconds\n",
    "timestamp_utc1 = timestamp_pt1 + pt_offset_seconds\n",
    "\n",
    "# Convertir a objeto datetime en PT y en UTC\n",
    "datetime_pt0 = datetime.datetime.fromtimestamp(timestamp_pt0 / 1e6)\n",
    "datetime_utc0 = datetime.datetime.utcfromtimestamp(timestamp_utc0 / 1e6)\n",
    "\n",
    "datetime_pt1 = datetime.datetime.fromtimestamp(timestamp_pt1 / 1e6)\n",
    "datetime_utc1 = datetime.datetime.utcfromtimestamp(timestamp_utc1 / 1e6)\n",
    "\n",
    "\n",
    "#print(\"inicial Tiempo en PT:\", datetime_pt0)\n",
    "print(\"inicial Tiempo en UTC:\", datetime_utc0)\n",
    "\n",
    "#print(\"final Tiempo en PT:\", datetime_pt1)\n",
    "print(\"final Tiempo en UTC:\", datetime_utc1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading templates\n",
    "\n",
    "template_list = glob.glob('/data/data4/veronica-scratch-rainier/templates-files/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee00c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory to save files\n",
    "base_directory = '/data/data4/veronica-scratch-rainier/test_corr'\n",
    "#the folder contain in the name_numbers date of the template used\n",
    "\n",
    "#base_directory = '/home/velgueta/notebooks/RainierDas' #test\n",
    "\n",
    "corrs = \"corrs\"\n",
    "#temp = \"informacion de la ultima parte de cada template\"\n",
    "\n",
    "daystart = file_list[0]\n",
    "dayend = file_list[-1]\n",
    "\n",
    "# Extract date, hour, and minute portion from file path, excluding \"new\" and \"decimator\"\n",
    "daystart_parts = os.path.basename(daystart).split('_')\n",
    "daystart_date_time = \"_\".join(daystart_parts[2:4])  # Extract the date and time parts, excluding \"new_decimator\" and \"UTC.h5\"\n",
    "\n",
    "dayend_parts = os.path.basename(dayend).split('_')\n",
    "dayend_date_time = \"_\".join(dayend_parts[2:4])  # Extract the date and time parts, excluding \"new_decimator\" and \"UTC.h5\"\n",
    "\n",
    "chan_min = 1000\n",
    "chan_max = 2500\n",
    "channel_number = chan_max -chan_min\n",
    "low_cut1 = 2\n",
    "hi_cut1 = 9.0\n",
    "#fs=attrs['MaximumFrequency']*2\n",
    "fs = 20\n",
    "samples_per_file = 60*fs\n",
    "b, a = butter(2, (low_cut1, hi_cut1), 'bp', fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd1af0",
   "metadata": {},
   "source": [
    "# Buiding outputfiles and correlations for each template on the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94dc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "for i, file in tqdm(enumerate(file_list)):\n",
    "    for j, tem in tqdm(enumerate(template_list)):\n",
    "        try:\n",
    "            # Load data file and template\n",
    "            with h5py.File(file, \"r\") as f, h5py.File(tem, \"r\") as d:\n",
    "                template = np.array(d['Acquisition/Raw[0]/RawData'][:, 0:-1])\n",
    "                raw_data = np.array(f['Acquisition/Raw[0]/RawData'][:, chan_min:chan_max-1])\n",
    "                timestamps = np.array(f['Acquisition/Raw[0]/RawDataTime'])\n",
    "                \n",
    "                # Filter data\n",
    "                data_filt = filtfilt(b, a, raw_data, axis=0)\n",
    "                \n",
    "                # Compute correlations\n",
    "                corrs = window_and_correlate(template, data_filt)\n",
    "                corrs2 = corrs.reshape((int(samples_per_file), channel_number-1))\n",
    "                corrs3 = np.sum(corrs2, axis=1) / channel_number\n",
    "                \n",
    "                # Create output folder name\n",
    "                folder_name_parts = os.path.splitext(os.path.basename(tem))[0].split('_')[:2]\n",
    "                folder_name = '_'.join(folder_name_parts)\n",
    "                folder_output = os.path.join(base_directory, folder_name)\n",
    "                \n",
    "                # Create folder if it doesn't exist\n",
    "                if not os.path.exists(folder_output):\n",
    "                    os.mkdir(folder_output)\n",
    "                \n",
    "                # Save correlation results\n",
    "                outfile_name = os.path.join(folder_output, f'corrs_{i}_.npy')\n",
    "                np.save(outfile_name, corrs3)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file} with template {tem}: {e}. Skipping this file and moving to the next template.\")\n",
    "            continue\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"The code took {execution_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf96c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time vector for plotting correlations\n",
    "\n",
    "#time steps\n",
    "#timesteps = len(file_list)* samples_per_file\n",
    "# Extraer las fechas de inicio y fin del nombre de la carpeta de salida\n",
    "\n",
    "#just formatting\n",
    "\n",
    "# Convertir las cadenas de tiempo a objetos datetime\n",
    "#start_datetime = pd.to_datetime(daystart_date_time, format=\"%Y-%m-%d_%H.%M.%S\")\n",
    "#end_datetime = pd.to_datetime(dayend_date_time, format=\"%Y-%m-%d_%H.%M.%S\")\n",
    "\n",
    "#final vector\n",
    "#time_range = pd.date_range(start=start_datetime, end=end_datetime, periods=timesteps)\n",
    "import datetime\n",
    "\n",
    "# Suponiendo que tienes daystart_date_time y dayend_date_time definidos como cadenas de tiempo\n",
    "\n",
    "# Calcular el número total de pasos de tiempo\n",
    "timesteps = len(file_list) * samples_per_file\n",
    "\n",
    "# Convertir las cadenas de tiempo a objetos datetime\n",
    "start_datetime = datetime.datetime.strptime(daystart_date_time, \"%Y-%m-%d_%H.%M.%S\")\n",
    "end_datetime = datetime.datetime.strptime(dayend_date_time, \"%Y-%m-%d_%H.%M.%S\")\n",
    "\n",
    "# Crear el rango de fechas o tiempo\n",
    "time_range = [start_datetime + i * (end_datetime - start_datetime) / (timesteps - 1) for i in range(timesteps)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test para un tiempo mas preciso\n",
    "\n",
    "# Marca de tiempo inicial y final en UTC\n",
    "start_time_utc = datetime_utc0\n",
    "end_time_utc = datetime_utc1\n",
    "\n",
    "# Calcular el número total de pasos de tiempo\n",
    "timesteps = (len(file_list)) * (samples_per_file)\n",
    "\n",
    "# Crear el rango de fechas o tiempo\n",
    "time_range = [start_time_utc + i * (end_time_utc - start_time_utc) / (timesteps - 1) for i in range(timesteps)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d36dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Directorio base donde se encuentran las carpetas\n",
    "base_directory = '/data/data4/veronica-scratch-rainier/test_corr'\n",
    "\n",
    "# Obtener una lista de todas las carpetas en el directorio base\n",
    "folders = [folder for folder in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, folder))]\n",
    "\n",
    "# Inicializar una lista para almacenar los datos concatenados por cada carpeta\n",
    "concatenated_data_per_folder = []\n",
    "\n",
    "# Iterar sobre las carpetas y cargar los archivos .npy\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_directory, folder)\n",
    "    npy_files = [np.load(os.path.join(folder_path, file)) for file in os.listdir(folder_path) if file.endswith('.npy')]\n",
    "    concatenated_data_per_folder.append(np.concatenate(npy_files, axis=0))\n",
    "\n",
    "# Crear el rango de fechas o tiempo con exactamente 20 objetos datetime por segundo\n",
    "# Asumiendo que ya tienes definido time_range adecuadamente\n",
    "\n",
    "# Graficar los datos para cada carpeta\n",
    "fig, axs = plt.subplots(len(folders), 1, figsize=(10, 5*len(folders)))\n",
    "\n",
    "for i, folder_data in enumerate(concatenated_data_per_folder):\n",
    "    axs[i].plot(time_range, folder_data, label=f'Folder {folders[i]}', color='blue', linestyle='-')\n",
    "    axs[i].set_title(f'Template {folders[i]}')\n",
    "    axs[i].set_xlabel('Time')\n",
    "    axs[i].set_ylabel('Correlation Value')\n",
    "    axs[i].legend()  # Add legend to each subplot\n",
    "    axs[i].set_ylim([0,0.5])  # Set y-axis limits from 0 to 0.5\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7efea5",
   "metadata": {},
   "source": [
    "# add thresold and detections stuff"
   ]
  },
  {
   "cell_type": "raw",
   "id": "599d5266",
   "metadata": {},
   "source": [
    "#the code took 9 hr for 15 templates, pretty efficient!\n",
    "#before we have 6 hour by template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52415f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define los umbrales\n",
    "thresholds = [0.5, 0.1, 0.05, 0.04, 0.035, 0.03]\n",
    "\n",
    "# Directorio donde se guardarán los archivos de texto\n",
    "output_directory = \"textfiles_results_thresholds\"\n",
    "\n",
    "# Itera sobre cada carpeta\n",
    "for folder, folder_data in zip(folders, concatenated_data_per_folder):\n",
    "    # Crear directorio si no existe\n",
    "    folder_output_directory = os.path.join(output_directory, folder)\n",
    "    if not os.path.exists(folder_output_directory):\n",
    "        os.makedirs(folder_output_directory)\n",
    "    \n",
    "    # Ruta del archivo de salida\n",
    "    output_file_path = os.path.join(folder_output_directory, f\"detections_results_{folder}.txt\")\n",
    "    \n",
    "    # Abre el archivo de salida en modo escritura\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        # Escribe los encabezados\n",
    "        file.write(\"Threshold\\tNumber of Detections\\tDetection Times (UTC)\\n\")\n",
    "\n",
    "        # Itera sobre cada umbral\n",
    "        for threshold in thresholds:\n",
    "            # Encuentra los índices donde los valores superan el umbral\n",
    "            indices_above_threshold = np.where(np.abs(folder_data) > threshold)[0]\n",
    "            diff_indices = np.diff(indices_above_threshold)\n",
    "            group_changes = np.where(diff_indices > 10)[0]\n",
    "            detection_groups = np.split(indices_above_threshold, group_changes + 1)\n",
    "\n",
    "            # Itera sobre cada grupo de detección\n",
    "            for group in detection_groups:\n",
    "                # Verifica si el grupo no está vacío\n",
    "                if len(group) > 0:\n",
    "                    # Toma solo la primera fecha en cada grupo\n",
    "                    first_detection_time_utc = time_range[group[0]].strftime('%Y-%m-%d_%H.%M.%S')\n",
    "                    #first_detection_time_utc = datetime.datetime.utcfromtimestamp(time_range[group[0]].timestamp()).strftime('%Y-%m-%d_%H.%M.%S')\n",
    "                    \n",
    "                    # Escribe los datos en una fila separados por tabulaciones\n",
    "                    file.write(f\"{threshold}\\t{len(detection_groups)}\\t{first_detection_time_utc}\\n\")\n",
    "                else:\n",
    "                    print(\"Grupo vacío encontrado para el umbral:\", threshold)\n",
    "\n",
    "# Mensaje de confirmación\n",
    "print(f\"Los resultados se han guardado en {output_directory}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Obspy+LibComtCat)",
   "language": "python",
   "name": "python-obspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
